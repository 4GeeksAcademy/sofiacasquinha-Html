{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here\n",
                "\n",
                "It's recommended to use this notebook for exploration purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "200\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from bs4 import BeautifulSoup\n",
                "import requests\n",
                "import time\n",
                "import sqlite3\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "\n",
                "#Step 2: Download HTML\n",
                "# The HTML of the web page will be downloaded using the requests library, as we saw in the module's theory.\n",
                "\n",
                "# The web page we want to scrape is the following: https://en.wikipedia.org/wiki/List_of_most-streamed_songs_on_Spotify. Collect and store the scraped text from the web in a variable.\n",
                "url = \"https://en.wikipedia.org/wiki/List_of_most-streamed_songs_on_Spotify\"\n",
                "resposta = requests.get(url)\n",
                "if resposta:\n",
                "    soup = BeautifulSoup(resposta.text, 'html.parser')\n",
                "    scraped = soup.get_text()\n",
                "\n",
                "print(resposta.status_code)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Step 3: Transform the HTML\n",
                "# Using BeautifulSoup, analyze the HTML to find the structure containing the data (e.g., <table>, <li>, <div>, etc.).\n",
                "\n",
                "# If you are using Wikipedia and it contains a table, you can directly use pandas.read_html() to load it as a DataFrame.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Step 4: Process the DataFrame\n",
                "# Next, clean the rows to obtain clean values by removing $ and B. Also, remove any rows that are empty or lack information.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Step 5: Store the data in SQLite\n",
                "# Create an empty database instance and include the cleaned data in it, as we saw in the database module. Once you have an empty database:\n",
                "\n",
                "# Create the table.\n",
                "# Insert the values.\n",
                "# Commit the changes.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Step 6: Visualize the data (optional, but highly recommended)\n",
                "# If you havenâ€™t gone through the visualization concepts and practices yet, donâ€™t worry. Try making this work, and weâ€™ll explore visualization in depth in the next few projects.\n",
                "\n",
                "# What types of visualizations can we make? Suggest at least 3 and plot them.\n",
                "\n",
                "# Do You Feel Confident? ðŸ˜Ž\n",
                "# Daily Monitoring of Music Rankings - Extended Version for Confident Students\n",
                "# If you feel confident and want to deepen your skills in web scraping and temporal data analysis, we propose this extended and optional version of the project. It will help you connect scraping, real data analysis, and professional visibility, making it ideal to showcase on LinkedIn or in a portfolio.\n",
                "\n",
                "# The idea is to collect daily information about music rankings (such as Spotify's top 100 songs from Wikipedia) and study real trends over time.\n",
                "\n",
                "# Proposal ðŸš€\n",
                "# Daily Scraper: Use the scraper from the original project. Schedule it to run daily (you can use cron on Linux/Mac or Task Scheduler on Windows).\n",
                "\n",
                "# Store the data in an SQLite database, adding a date column with the execution day.\n",
                "\n",
                "# Database: Create a table called daily_rankings with the following columns:\n",
                "\n",
                "# scraping_date\n",
                "# rank\n",
                "# song\n",
                "# artist\n",
                "# streams\n",
                "# release_year\n",
                "# Suggested Visualizations:\n",
                "\n",
                "# Evolution of a song over time\n",
                "# Average time spent in the top 10, top 50, or top 100\n",
                "# Artists with the most entries and highest average duration\n",
                "# Make Your Work Visible: Share your work on LinkedIn. Track the data for at least 2 weeks and publish a visualization or insight daily or every 2-3 days. Use notebooks, dashboards, or graphical posts to share what youâ€™ve learned.\n",
                "\n",
                "# Hereâ€™s a suggested LinkedIn post:\n",
                "\n",
                "# Among my first projects as a Data Scientist, today I started monitoring the daily rankings of the most-streamed songs on Spotify. Reviewing these rankings is key to understanding a lot about how money, marketing, and trends move in the music industry.\n",
                "\n",
                "# Iâ€™ll be sharing my visualizations and insights in the coming days.\n",
                "\n",
                "# Music can also be studied with data! ðŸŽ¶ðŸ“Š\n",
                "\n",
                "#DataScience #Spotify #WebScraping #MusicTrends"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
